MetaModel means CMA equipped with a MetaModel, which is automatically enabled when the learning performs well.
ChainMetaModelSQP is a memetic algorithm: it combines MetaModel (hence, CMA plus a MetaModel) and a final run using a local method, namely SQP (sequential quadratic programming).
Usually, a meta-model and a fast local search a the end do improve evolutionary methods.

On benchmarks close to the good old BBOB, CMA and variants do perform well. The variants equipped with a MetaModel perform better, and
variants equipped with the the MetaModel and the final local search are even better.

On many real-world benchmarks, the budget is lower than in the traditional context of BBOB with budget $=$ dimension $\times$ 1000. There
are cases with a ratio budget/dimension $<1$, of the order of a few units or a few dozens. DE performs well in many cases. This is consistent with many publications considering real-world problems. Our real-world benchmarks include TODO

SQP is excellent in noisy optimization. In the large-scale context, TBPSA (combining population-control\cite{mlis} and other tools as in \cite{vasilfoga}) also performs well.
Methods designed in the noisy case might diverge.

We include benchmarks with one or several or many constraints (prefix onepen, pen and megapen), tackled with dynamic penalization: results were not fundamentally different from the non-penalized case. However, MetaModels are effective in a very stable and visible manner: this is consistent with the state of the art TODO

MetaModel perform well on BBOB-style optimization, but were also excellent for several low budget things.

Regarding discrete contexts, we note the great performance of the so-called ``DiscreteLenglerOnePlusOne'' \cite{lengler}. We tested variants with a different constant and it turns out that the proved constant, in spite of the simple context in which it was derived, is good.
Still in the discrete case, we note the good performance of methods with ``Recombining'' in the name: while it was less investigated theoretically than variants of the discrete $(1+1)$ method, methods with crossover might deserve more work.

Regarding the principles of benchmarking, we note that the two different views in Nevergrad (the heatmap and the average normalized loss) present completely different views. This emphasizes how much how we look at data has a big impact on the interpretation.

Regarding BBOB variants, TODO

Regarding PBBOB, TODO

Regarding Holland and Voronoi crossovers, TODO

Algorithms taking into account the difference between ordered and unordered discrete variables TODO

In the continuous low budget case, Cobyla and BOBYQA are frequently excellent in moderate dimension. The high-dimensional case sees great successes of DE variants.

Consistently with some real world experiments in \cite{micropredictions1,micropredictions2}, we note the the $(1+1)$ evolution strategy with one-fifth rule from \cite{rechenberg73} is still quite good. In artificial benchmarks with a lot of evaluations, high conditionning and artificially rotated contexts, it can become weak: for many realistic contexts, in particular a realistic ratio budget/dimension, it is quite good.

The simple $(1+1)$ evolution strategy with one-fifth rule, when equipped with a meta-model, turned out to be powerful.
New memetic TODO

In real-world benchmark tunings, results varied vastly from one benchmark to the next.  It looks like we can conclude
anything we want by selecting a benchmark or by tuning algorithms for a specific case.

The multi-objective setting is quite difficult to analyze. The DE adapted to the multi-objective case available in
Nevergrad performs well in classical settings, but high dimensional and many objective cases lead to surprisingly good
results  by e.g. discrete methods running in the continuous case (which means they are handled that variables with
infinitely many possibles values).

\subsection{Caveats, further work}
Some benchmarks were implemented but not included in the release due to legal issues in the license. 
We did not include the important case in which a multi-objective run is performed on surrogate models only: (1) randomly sample, (2) approximate the objective functions by surrogate models, (3) perform a multi-objective optimization on the surrogate only. This is useful for including the user in the loop. This is not tested in the current benchmarks.

Compared to the old Dashboard from 2021, results are somehow similar, with more details. However, we have more real world benchmarks and more discrete experiments. Also,
some methods have been removed, in particular some slow methods which were rarely performing well compared to present methods.
