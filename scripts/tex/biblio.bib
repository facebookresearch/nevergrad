@misc{bbobissue1,
author={Remi Coulom},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000257.html}},
year={2012}} 
@misc{bbobissue2,
author={Hans-Georg Beyer},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000270.html}},
year={2012}}
@misc{bbobissue3,
author={Hans-Georg Beyer},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000258.html}},
year={2012}
}
@misc{bbobissue4,
author={Remi Coulom},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000252.html}},
year={2012}
}

@inproceedings{decocknoise,
 author = {Decock, J{\'e}r{\'e}mie and Teytaud, Olivier},
 title = {Noisy Optimization Complexity Under Locality Assumption},
 booktitle = {Proceedings of the Twelfth Workshop on Foundations of Genetic Algorithms XII},
 series = {FOGA XII '13},
 year = {2013},
 unusedisbn =  {978-1-4503-1990-4},
 unusedunusedlocation = {Adelaide, Australia},
 pages = {183--190},
 numpages = {8},
 unusedurl = {http://doi.acm.org/10.1145/2460239.2460256},
 unusedunuseddoi =  {10.1145/2460239.2460256},
 acmid = {2460256},
 publisher = {ACM},
 unusedunusedaddress = {New York, NY, USA},
 unusedkeywords = {black box complexity model, local sampling, noisy optimization},
} 
@article{fabian,
author = "Fabian, Vaclav",
unusedunuseddoi =  "10.1214/aoms/1177699070", 
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "02",
number = "1",  
pages = "191--200",
publisher = "The Institute of Mathematical Statistics",
title = "Stochastic Approximation of Minima with Improved Asymptotic Speed",
unusedurl = "https://doi.org/10.1214/aoms/1177699070",
volume = "38",
year = "1967"
}

   

@article{chen1988, 
        author = "Chen, Hung",
        unusedunuseddoi =  "10.1214/aos/1176350965",
        journal = "The Annals of Statistics",
        month = "Sep",
        number = "3",
        pages = "1330--1334", 
        publisher = "The Institute of Mathematical Statistics",
        title = "Lower Rate of Convergence for Locating a Maximum of a Function",
        unusedurl = "http://dx.doi.org/10.1214/aos/1176350965",
        volume = "16",
        year = "1988"
}


@misc{micropredictions2,
   author = {MicroPredictions}, 
   author = {Petter Cotton},
   title = {MicroPredictions ELO ratings},
   year = "2020",    
   howpublished =  {\url{https://microprediction.github.io/optimizer-elo-ratings/}},
   unusednote = "[Online; accessed 27-April-2021]",
}       
        
@misc{micropredictions1,
   author = {Petter Cotton},
   title = {An introduction to Z-streams (and collective micropredictions)},
   year = "2020",
   howpublished =  {\url{https://www.linkedin.com/pulse/short-introduction-z-streams-peter-cotton-phd/}},
   unusednote = "[Online; accessed 27-March-2021]"
 }      

@book{rechenberg73,
        title        = {Evolutionstrategie: Optimierung Technischer Systeme nach Prinzipien des Biologischen Evolution},
        author       = {Ingo Rechenberg},
        year         = 1973,
        publisher    = {Fromman-Holzboog Verlag},
        unusedaddress = {Stuttgart}
}


@article{relengler,
author       = {Hafsteinn Einarsson and
  Marcelo Matheus Gauy and
  Johannes Lengler and
  Florian Meier and
  Asier Mujika and
  Angelika Steger and
  Felix Weissenberger},
title = {The linear hidden subset problem for the (1+1)-{EA} with scheduled and adaptive mutation rates},
journal = {Theor.  Comput.  Sci.}, 
volume = {785}, 
pages = {150--170},
year = {2019},
url = {https://doi.org/10.1016/j.tcs.2019.05.021},
doi = {10.1016/j.tcs.2019.05.021},
timestamp = {Tue, 03 Sep 2019 16:31:11 +0200},
biburl = {https://dblp.org/rec/journals/tcs/EinarssonGLMMSW19.bib}, 
bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{lengler,
author = {Doerr, Benjamin and Doerr, Carola and Lengler, Johannes},
title = {Self-Adjusting Mutation Rates with Provably Optimal Success Rules},
year = {2019},
unusedisbn = {9781450361118},
publisher = {Association for Computing Machinery},
unusedaddress = {New York, NY, USA},
unusedurl = {https://doi.org/10.1145/3321707.3321733},
unusedunuseddoi = {10.1145/3321707.3321733},
abstract = {The one-fifth success rule is one of the best-known and most widely accepted techniques to control the parameters of evolutionary algorithms. While it is often applied in the literal sense, a common interpretation sees the one-fifth success rule as a family of success-based updated rules that are determined by an update strength F and a success rate s. We analyze in this work how the performance of the (1+1) Evolutionary Algorithm (EA) on LeadingOnes depends on these two hyper-parameters. Our main result shows that the best performance is obtained for small update strengths F = 1+o(1) and success rate 1/e. We also prove that the runtime obtained by this parameter setting is asymptotically optimal among all dynamic choices of the mutation rate for the (1+1) EA, up to lower order error terms. We show similar results for the resampling variant of the (1+1) EA, which enforces to flip at least one bit per iteration.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1479â€“1487}, 
numpages = {9},
unusedlocation = {Prague, Czech Republic},
series = {GECCO '19} 
}

@inproceedings{lamcts,
  author    = {Linnan Wang and
               Rodrigo Fonseca and
               Yuandong Tian},
  title     = {Learning Search Space Partition for Black-box Optimization using Monte
               Carlo Tree Search},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/e2ce14e81dba66dbff9cbc35ecfdb704-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:19 +0100},
,
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
 
@misc{leakage,
  doi = {10.48550/ARXIV.2207.07048},

  url = {https://arxiv.org/abs/2207.07048},

  author = {Kapoor, Sayash and Narayanan, Arvind},

  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Leakage and the Reproducibility Crisis in ML-based Science},

  publisher = {arXiv},

  year = {2022},

  copyright = {arXiv.org perpetual, non-exclusive license}
}
      
      
@misc{rlgoogle,
      title={The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement},
      author={Igor L. Markov},
      year={2023},
      eprint={2306.09633},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
} 
      


@misc{ecnassurvey,
      title={A Survey on Evolutionary Neural Architecture Search},
      author={Yuqiao Liu and Yanan Sun and Bing Xue and Mengjie Zhang and Gary G. Yen and Kay Chen Tan},
      year={2021},
      eprint={2008.10937}, 
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{repronas,
      title={Random Search and Reproducibility for Neural Architecture Search},
      author={Liam Li and Ameet Talwalkar},
      year={2019},
      eprint={1902.07638},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{pham2018efficient,
      title={Efficient Neural Architecture Search via Parameter Sharing},
      author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
      year={2018},
      eprint={1802.03268},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{real2019regularized,
      title={Regularized Evolution for Image Classifier Architecture Search},
      author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V Le},
      year={2019},
      eprint={1802.01548},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@InProceedings{mlis,
author="Beyer, Hans-Georg",
title="Mutate large, but inherit small! On the analysis of rescaled mutations in (                                                          $( 1,\lambda)$-ES with noisy fitness data",
booktitle="Parallel Problem Solving from Nature --- PPSN V",
year="1998", 
publisher="Springer",
unusedunusedaddress="Berlin, Heidelberg",
pages="109--118",
abstract="The paper presents the asymptotical analysis of a technique for improving the convergence of evolution strategies (ES) on noisy fitness data. This technique that may be called ``Mutate large, but inherit small'', is discussed in light of the EPP (evolutionary progress principle). The derivation of the progress rate formula is sketched, its predictions are compared with experiments, and its limitations are shown. The dynamical behavior of the ES is investigated. It will be shown that standard self-adaptation has considerable problems to drive the ES in its optimum working regime. Remedies are provided to improve the self-adaptation.",
unusedisbn="978-3-540-49672-4"
}

@inproceedings{vasilfoga,
  author    = {Vasil Khalidov and
               Maxime Oquab and
               J{\'{e}}r{\'{e}}my Rapin and
               Olivier Teytaud},
  title     = {Consistent population control: generate plenty of points, but with
               a bit of resampling},
  booktitle = {Proceedings of the 15th {ACM/SIGEVO} Conference on Foundations of
               Genetic Algorithms, {FOGA} 2019, Potsdam, Germany, August 27-29, 2019},
  pages     = {116--123},
  year      = {2019},
  crossref  = {DBLP:conf/foga/2019},
  url       = {https://doi.org/10.1145/3299904.3340312},
  unuseddoi       = {10.1145/3299904.3340312},
}   

@INPROCEEDINGS{pde,
  author={Abbass, H.A. and Sarker, R. and Newton, C.},
    booktitle={Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)}, 
      title={PDE: a Pareto-frontier differential evolution approach for multi-objective optimization problems}, 
        year={2001},
          volume={2},
            number={},
              pages={971-978 vol. 2},
                doi={10.1109/CEC.2001.934295}}


@inproceedings{mode,
  title={DEMO: Differential Evolution for Multiobjective Optimization},
    author={Tea Robic and Bogdan Filipi},
      booktitle={International Conference on Evolutionary Multi-Criterion Optimization},
        year={2005}
        }

@inproceedings{quasiopposite,
    author = "{Rahnamayan}, S. and {Tizhoosh}, H. R. and {Salama}, M. M. A.",
        title = "Quasi-oppositional Differential Evolution",
            year = "2007",
                month = "Sep.",
                    booktitle = "2007 IEEE Congress on Evolutionary Computation",
                        volume = "",
                            number = "",
                                pages = "2229--2236",
                                    keywords = "evolutionary computation;quasioppositional differential
                                    evolution;black-box optimization;global optimization algorithm;Acceleration;Pattern
                                    analysis;Machine intelligence;Genetic mutations;Biomedical
                                    engineering;Instruments;Benchmark testing;Robust control;Optimization
                                    methods;Convergence",
                                        unuseddoi = "10.1109/CEC.2007.4424748",
                                            unusedissn = ""
                                            }


@misc{bbochallenge,
title={Black-box Optimization Challenge},
year={2021},
howpublished={\url{https://bbochallenge.com/altleaderboard}},
}

@misc{squirrel,
      title={Squirrel: A Switching Hyperparameter Optimizer}, 
            author={Noor Awad and Gresa Shala and Difan Deng and Neeratyoy Mallik and Matthias Feurer and Katharina
            Eggensperger and Andre' Biedenkapp and Diederick Vermetten and Hao Wang and Carola Doerr and Marius Lindauer
            and Frank Hutter},
                  year={2020},
                        eprint={2012.08180},
                              archivePrefix={arXiv},
                                    primaryClass={cs.LG}
                                    }

