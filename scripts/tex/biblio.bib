@misc{bbobissue1,
author={Remi Coulom},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000257.html}},
year={2012}} 
@misc{bbobissue2,
author={Hans-Georg Beyer},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000270.html}},
year={2012}}
@misc{bbobissue3,
author={Hans-Georg Beyer},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000258.html}},
year={2012}
}
@misc{bbobissue4,
author={Remi Coulom},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000252.html}},
year={2012}
}

@inproceedings{decocknoise,
 author = {Decock, J{\'e}r{\'e}mie and Teytaud, Olivier},
 title = {Noisy Optimization Complexity Under Locality Assumption},
 booktitle = {Proceedings of the Twelfth Workshop on Foundations of Genetic Algorithms XII},
 series = {FOGA XII '13},
 year = {2013},
 unusedisbn =  {978-1-4503-1990-4},
 unusedunusedlocation = {Adelaide, Australia},
 pages = {183--190},
 numpages = {8},
 unusedurl = {http://doi.acm.org/10.1145/2460239.2460256},
 unusedunuseddoi =  {10.1145/2460239.2460256},
 acmid = {2460256},
 publisher = {ACM},
 unusedunusedaddress = {New York, NY, USA},
 unusedkeywords = {black box complexity model, local sampling, noisy optimization},
} 
@article{fabian,
author = "Fabian, Vaclav",
unusedunuseddoi =  "10.1214/aoms/1177699070", 
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "02",
number = "1",  
pages = "191--200",
publisher = "The Institute of Mathematical Statistics",
title = "Stochastic Approximation of Minima with Improved Asymptotic Speed",
unusedurl = "https://doi.org/10.1214/aoms/1177699070",
volume = "38",
year = "1967"
}

   

@article{chen1988, 
        author = "Chen, Hung",
        unusedunuseddoi =  "10.1214/aos/1176350965",
        journal = "The Annals of Statistics",
        month = "Sep",
        number = "3",
        pages = "1330--1334", 
        publisher = "The Institute of Mathematical Statistics",
        title = "Lower Rate of Convergence for Locating a Maximum of a Function",
        unusedurl = "http://dx.doi.org/10.1214/aos/1176350965",
        volume = "16",
        year = "1988"
}


@misc{micropredictions2,
   author = {MicroPredictions}, 
   author = {Petter Cotton},
   title = {MicroPredictions ELO ratings},
   year = "2020",    
   howpublished =  {\url{https://microprediction.github.io/optimizer-elo-ratings/}},
   unusednote = "[Online; accessed 27-April-2021]",
}       
        
@misc{micropredictions1,
   author = {Petter Cotton},
   title = {An introduction to Z-streams (and collective micropredictions)},
   year = "2020",
   howpublished =  {\url{https://www.linkedin.com/pulse/short-introduction-z-streams-peter-cotton-phd/}},
   unusednote = "[Online; accessed 27-March-2021]"
 }      

@book{rechenberg73,
        title        = {Evolutionstrategie: Optimierung Technischer Systeme nach Prinzipien des Biologischen Evolution},
        author       = {Ingo Rechenberg},
        year         = 1973,
        publisher    = {Fromman-Holzboog Verlag},
        unusedaddress = {Stuttgart}
}


@inproceedings{lengler,
author = {Doerr, Benjamin and Doerr, Carola and Lengler, Johannes},
title = {Self-Adjusting Mutation Rates with Provably Optimal Success Rules},
year = {2019},
unusedisbn = {9781450361118},
publisher = {Association for Computing Machinery},
unusedaddress = {New York, NY, USA},
unusedurl = {https://doi.org/10.1145/3321707.3321733},
unusedunuseddoi = {10.1145/3321707.3321733},
abstract = {The one-fifth success rule is one of the best-known and most widely accepted techniques to control the parameters of evolutionary algorithms. While it is often applied in the literal sense, a common interpretation sees the one-fifth success rule as a family of success-based updated rules that are determined by an update strength F and a success rate s. We analyze in this work how the performance of the (1+1) Evolutionary Algorithm (EA) on LeadingOnes depends on these two hyper-parameters. Our main result shows that the best performance is obtained for small update strengths F = 1+o(1) and success rate 1/e. We also prove that the runtime obtained by this parameter setting is asymptotically optimal among all dynamic choices of the mutation rate for the (1+1) EA, up to lower order error terms. We show similar results for the resampling variant of the (1+1) EA, which enforces to flip at least one bit per iteration.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1479â€“1487}, 
numpages = {9},
unusedlocation = {Prague, Czech Republic},
series = {GECCO '19} 
}

@inproceedings{lamcts,
  author    = {Linnan Wang and
               Rodrigo Fonseca and
               Yuandong Tian},
  title     = {Learning Search Space Partition for Black-box Optimization using Monte
               Carlo Tree Search},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/e2ce14e81dba66dbff9cbc35ecfdb704-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:19 +0100},
,
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
 
@misc{leakage,
  doi = {10.48550/ARXIV.2207.07048},

  url = {https://arxiv.org/abs/2207.07048},

  author = {Kapoor, Sayash and Narayanan, Arvind},

  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Leakage and the Reproducibility Crisis in ML-based Science},

  publisher = {arXiv},

  year = {2022},

  copyright = {arXiv.org perpetual, non-exclusive license}
}
      
      
@misc{rlgoogle,
      title={The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement},
      author={Igor L. Markov},
      year={2023},
      eprint={2306.09633},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
} 
      


@misc{ecnassurvey,
      title={A Survey on Evolutionary Neural Architecture Search},
      author={Yuqiao Liu and Yanan Sun and Bing Xue and Mengjie Zhang and Gary G. Yen and Kay Chen Tan},
      year={2021},
      eprint={2008.10937}, 
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{repronas,
      title={Random Search and Reproducibility for Neural Architecture Search},
      author={Liam Li and Ameet Talwalkar},
      year={2019},
      eprint={1902.07638},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{pham2018efficient,
      title={Efficient Neural Architecture Search via Parameter Sharing},
      author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
      year={2018},
      eprint={1802.03268},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{real2019regularized,
      title={Regularized Evolution for Image Classifier Architecture Search},
      author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V Le},
      year={2019},
      eprint={1802.01548},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

