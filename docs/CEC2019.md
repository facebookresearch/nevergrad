# CEC Nevergrad Competition: continuous optimization.

This competition consists in implementing derivative free algorithms using the `nevergrad` platform and benchmarking them on 5 different trackes. For each of these tracks, the CEC participant with the best results will be **awarded 560NZ$**.

**The deadline has been extended to June 2nd**.

## Installation

`nevergrad` uses Python3.6+. Here are the instructions to download and install it:

```
git clone https://github.com/facebookresearch/nevergrad.git
cd nevergrad
git fetch origin cec2019:cec2019
git checkout cec2019
python3 -m venv venv  # optional (using such a virtual environment is advised though)
source venv/bin/activate  # optional
pip3 install -r requirements.txt
```

Please note that this branch is not up-to-date with `master`. Thus, make sure you refer to the documentation from the `cec2019` branch since some elements may differ from `master`.

##Â Launching experiments

How to launch and generate results:
 - Choose a track `X` (see below).
 - Modify `CustomOptimizer` in the following file  (the default optimizer in `CustomOptimizer` will not win the competition!):
   `nevergrad/optimization/cec2019_optimizer.py` Please keep the name of the optimizer to `CustomOptimizer` since it is used
   for running the benchmarks. Also, feel free to have a look at currently implemented optimizers in
   [optimizerlib.py](../nevergrad/optimization/optimizerlib.py), [oneshot.py](../nevergrad/optimization/oneshot.py)
   or [differentialevolution.py](../nevergrad/optimization/differentialevolution.py) to help you implement your own optimizer.
   It is completely ok to use existing algorithms, as part of an algorithm selection method for example.
   Questions (see below) are also welcome.
 - Launch the following command (assuming you can use 4 threads):
   ```
   python -m nevergrad.benchmark X --num_workers=4 --plot
   ```
   This will run the experiments and generate a `<track_name>.csv` file containing the experiments data,
   and a `<track_name>_plots` folder with several figures, including  `fight_all.png` that will serve as criterion.
   You can run several times for improving the precision since the .csv file will be appended
   (corollary: you need to delete this file if you want to experiment with a new version of the optimizer).
   Check the repository's [README](../README.md) or use the `-h` argument for more information about this command
   (including the `--repetitions` keyword if you want to run the experiment several times).

## Sending results

Send to `oteytaud+cec2019@fb.com` and `jrapin+cec2019@fb.com` a `.tar.gz` or `.zip` or `.rar` file including your optimizer and generated plots and data, with the following format:
```
<your_choice_of_name>.zip/.tar.gz/.rar
|-> cec2019_optimizer.py
|-> <track_name>.csv
|-> <track_name>_plots
     |-> fight_all.png
     |-> etc...
```
Optionally, you may provide several tracks with the same structure if the same optimizer applies, but please send another
`.zip`/`.tar.gz`/`.rar` if you want to submit another optimizer.
The deadline is **June 1st**.

## The tracks

The tracks are as follows:
 - Deceptive functions: `X=deceptivecec`
 - One-shot optimization: `X=oneshotcec`
 - Noisy optimization: `X=noisycec`
 - Ill conditioned optimization: `X=illcondicec`
 - Parallel optimization: `X=parallelcec`

## Questions
Open an issue on this repository if you encounter any problem.

## Winning
The competition will distinguish the best result, for each track. The result, for your optimizer, is the average
winning rate displayed in figure `fight_all.png`. This number is a comparison between your optimizer and the
various optimizers we have in the platform. If your optimizer does not appear in the row labels
(leftmost part of the picture), then it does not perform as well as the baselines.
The organizers will rerun the best methods with large numbers of replicates; methods with pathologically
large computation time, or resource requirement or suspicious code, might be excluded.

A priori, there is no comparison directly between the different submitted optimizers.
However, in case of ties, or if for some reason the comparison is not meaningful,
the organizers are free to decide that a set of submitted optimizers should be rerun together,
and use the results for deciding the winner. More generally, the organizers will take
into account any unexpected element that might impact the competition and can adapt the rules.




