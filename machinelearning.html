<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Examples - Nevergrad for machine learning &mdash; nevergrad  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Optimizers API Reference" href="optimizers_ref.html" />
    <link rel="prev" title="Parametrizing your optimization" href="parametrization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            nevergrad
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">How to perform optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrization.html">Parametrizing your optimization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Examples - Nevergrad for machine learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#optimization-of-continuous-hyperparameters-with-cma-pso-de-random-and-quasirandom">Optimization of continuous hyperparameters with CMA, PSO, DE, Random and QuasiRandom</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ask-and-tell-version">Ask and tell version</a></li>
<li class="toctree-l3"><a class="reference internal" href="#asynchronous-version-with-concurrent-futures">Asynchronous version with concurrent.futures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-of-mixed-continuous-and-discrete-hyperparameters">Optimization of mixed (continuous and discrete) hyperparameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#manual-parametrization">Manual parametrization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-of-parameters-for-reinforcement-learning">Optimization of parameters for reinforcement learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#examples-from-our-external-users">Examples from our external users</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optimizers_ref.html">Optimizers API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrization_ref.html">Parametrization API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Running algorithm benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="r.html">Examples - Nevergrad for R</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Examples of benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pyomo.html">Examples - Working with Pyomo model</a></li>
<li class="toctree-l1"><a class="reference internal" href="windows.html">Installation and configuration on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to Nevergrad</a></li>
<li class="toctree-l1"><a class="reference internal" href="opencompetition2020.html">Open Optimization Competition 2020</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">nevergrad</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Examples - Nevergrad for machine learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/machinelearning.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="examples-nevergrad-for-machine-learning">
<span id="machinelearning"></span><h1>Examples - Nevergrad for machine learning<a class="headerlink" href="#examples-nevergrad-for-machine-learning" title="Permalink to this heading"></a></h1>
<p>Let us assume that you have defined an objective function as in:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">myfunction</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">arg3</span><span class="p">,</span> <span class="n">arg4</span><span class="p">,</span> <span class="n">other_anything</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">accuracy</span>  <span class="c1"># something to minimize</span>
</pre></div>
</div>
<p>You should define how it must be instrumented, i.e. what are the arguments you want to optimize upon, and on which space they are defined. If you have both continuous and discrete parameters, you have a good initial guess, maybe just use <code class="code docutils literal notranslate"><span class="pre">TransitionChoice</span></code> for all discrete variables, <code class="code docutils literal notranslate"><span class="pre">Array</span></code> for all your continuous variables, and use <code class="code docutils literal notranslate"><span class="pre">PortfolioDiscreteOnePlusOne</span></code> as optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nevergrad</span> <span class="k">as</span> <span class="nn">ng</span>
<span class="c1"># instrument learning rate and number of layers, keep arg3 to 3 and arg4 to 4</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># log distributed between 0.001 and 1</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">TransitionChoice</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">parametrization</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">Instrumentation</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="n">arg4</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Make sure <code class="code docutils literal notranslate"><span class="pre">parametrization.value</span></code> holds your initial guess. It is automatically populated, but can be updated manually (just set <code class="code docutils literal notranslate"><span class="pre">value</span></code> to what you want). For more details on parametrization, see the <a class="reference internal" href="parametrization.html#parametrizing"><span class="std std-ref">parametrization section</span></a>.</p>
<p>The fact that you use (ordered) discrete variables through <code class="code docutils literal notranslate"><span class="pre">TransitionChoice</span></code> is not a big deal because by nature <code class="code docutils literal notranslate"><span class="pre">PortfolioDiscreteOnePlusOne</span></code> will ignore the order. This algorithm is quite stable.</p>
<p>If you have more budget, a cool possibility is to use <code class="code docutils literal notranslate"><span class="pre">Choice</span></code> for all discrete variables and then apply <code class="code docutils literal notranslate"><span class="pre">TwoPointsDE</span></code>. You might also compare this to <code class="code docutils literal notranslate"><span class="pre">DE</span></code> (classical differential evolution). This might need a budget in the hundreds.</p>
<p>If you want to double-check that you are not worse than random search, you might use <code class="code docutils literal notranslate"><span class="pre">RandomSearch</span></code>.</p>
<p>If you want something fully parallel (the number of workers can be equal to the budget), then you might use <code class="code docutils literal notranslate"><span class="pre">ScrHammersleySearch</span></code>, which includes the discrete case. Then, you should use <code class="code docutils literal notranslate"><span class="pre">TransitionChoice</span></code> rather than <code class="code docutils literal notranslate"><span class="pre">Choice</span></code>. This does not have the traditional drawback of grid search and should still be more uniform than random. By nature <code class="code docutils literal notranslate"><span class="pre">ScrHammersleySearch</span></code> will deal correctly with <code class="code docutils literal notranslate"><span class="pre">TransitionChoice</span></code> type for discrete variables.</p>
<p>If you are optimizing weights in reinforcement learning, you might use <code class="code docutils literal notranslate"><span class="pre">TBPSA</span></code> (high noise) or <code class="code docutils literal notranslate"><span class="pre">CMA</span></code> (low noise).</p>
<p>Below are 3 examples :</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>the optimization of continuous hyperparameters. It is also presented in an asynchronous setting. All other examples are based on the ask and tell interface, which can be synchronous or not but relies on the user for setting up asynchronicity.</p></li>
<li><p>the optimization of mixed (continuous and discrete) hyperparameters.</p></li>
<li><p>the optimization of parameters in a noisy setting, typically as in reinforcement learning.</p></li>
</ol>
</div></blockquote>
<section id="optimization-of-continuous-hyperparameters-with-cma-pso-de-random-and-quasirandom">
<h2>Optimization of continuous hyperparameters with CMA, PSO, DE, Random and QuasiRandom<a class="headerlink" href="#optimization-of-continuous-hyperparameters-with-cma-pso-de-random-and-quasirandom" title="Permalink to this heading"></a></h2>
<p>Let’s first define our test case:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nevergrad</span> <span class="k">as</span> <span class="nn">ng</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimization of continuous hyperparameters =========&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_and_return_test_error</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="mf">50.</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">))</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>

<span class="n">parametrization</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">300</span><span class="p">,))</span>  <span class="c1"># optimize on R^300</span>

<span class="n">budget</span> <span class="o">=</span> <span class="mi">1200</span>  <span class="c1"># How many trainings we will do before concluding.</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;RandomSearch&quot;</span><span class="p">,</span> <span class="s2">&quot;TwoPointsDE&quot;</span><span class="p">,</span> <span class="s2">&quot;CMA&quot;</span><span class="p">,</span> <span class="s2">&quot;PSO&quot;</span><span class="p">,</span> <span class="s2">&quot;ScrHammersleySearch&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>We will compare several algorithms (defined in <code class="code docutils literal notranslate"><span class="pre">names</span></code>).
<code class="code docutils literal notranslate"><span class="pre">RandomSearch</span></code> is well known, <code class="code docutils literal notranslate"><span class="pre">ScrHammersleySearch</span></code> is a quasirandom; these two methods
are fully parallel, i.e. we can perform the 1200 trainings in parallel.
<code class="code docutils literal notranslate"><span class="pre">CMA</span></code> and <code class="code docutils literal notranslate"><span class="pre">PSO</span></code> are classical optimization algorithms, and <code class="code docutils literal notranslate"><span class="pre">TwoPointsDE</span></code>
is Differential Evolution equipped with a 2-points crossover.
A complete list is available in <code class="code docutils literal notranslate"><span class="pre">ng.optimizers.registry</span></code>.</p>
<section id="ask-and-tell-version">
<h3>Ask and tell version<a class="headerlink" href="#ask-and-tell-version" title="Permalink to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">registry</span><span class="p">[</span><span class="n">name</span><span class="p">](</span><span class="n">parametrization</span><span class="o">=</span><span class="n">parametrization</span><span class="p">,</span> <span class="n">budget</span><span class="o">=</span><span class="n">budget</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">budget</span> <span class="o">//</span> <span class="mi">3</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="c1"># Ask and tell can be asynchronous.</span>
        <span class="c1"># Just be careful that you &quot;tell&quot; something that was asked.</span>
        <span class="c1"># Here we ask 3 times and tell 3 times in order to fake asynchronicity</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="c1"># The three folowing lines could be parallelized.</span>
        <span class="c1"># We could also do things asynchronously, i.e. do one more ask</span>
        <span class="c1"># as soon as a training is over.</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">train_and_return_test_error</span><span class="p">(</span><span class="o">*</span><span class="n">x1</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">x1</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># here we only defined an arg, so we could omit kwargs</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">train_and_return_test_error</span><span class="p">(</span><span class="o">*</span><span class="n">x2</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">x2</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># (keeping it here for the sake of consistency)</span>
        <span class="n">y3</span> <span class="o">=</span> <span class="n">train_and_return_test_error</span><span class="p">(</span><span class="o">*</span><span class="n">x3</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">x3</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">y3</span><span class="p">)</span>
    <span class="n">recommendation</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">recommend</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;* &quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">&quot; provides a vector of parameters with test error &quot;</span><span class="p">,</span>
          <span class="n">train_and_return_test_error</span><span class="p">(</span><span class="o">*</span><span class="n">recommendation</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">recommendation</span><span class="o">.</span><span class="n">kwargs</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="asynchronous-version-with-concurrent-futures">
<h3>Asynchronous version with concurrent.futures<a class="headerlink" href="#asynchronous-version-with-concurrent-futures" title="Permalink to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">concurrent</span> <span class="kn">import</span> <span class="n">futures</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">registry</span><span class="p">[</span><span class="n">name</span><span class="p">](</span><span class="n">parametrization</span><span class="o">=</span><span class="n">instru</span><span class="p">,</span> <span class="n">budget</span><span class="o">=</span><span class="n">budget</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">num_workers</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>  <span class="c1"># the executor will evaluate the function in multiple threads</span>
        <span class="n">recommendation</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">train_and_return_test_error</span><span class="p">,</span> <span class="n">executor</span><span class="o">=</span><span class="n">executor</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;* &quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">&quot; provides a vector of parameters with test error &quot;</span><span class="p">,</span>
          <span class="n">train_and_return_test_error</span><span class="p">(</span><span class="o">*</span><span class="n">recommendation</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">recommendation</span><span class="o">.</span><span class="n">kwargs</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="optimization-of-mixed-continuous-and-discrete-hyperparameters">
<h2>Optimization of mixed (continuous and discrete) hyperparameters<a class="headerlink" href="#optimization-of-mixed-continuous-and-discrete-hyperparameters" title="Permalink to this heading"></a></h2>
<p>Let’s define our function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Let us define a function.</span>
<span class="k">def</span> <span class="nf">myfunction</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">arg1</span> <span class="o">!=</span> <span class="s2">&quot;a&quot;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">arg2</span> <span class="o">!=</span> <span class="s2">&quot;e&quot;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>This function must then be instrumented in order to let the optimizer now what are the arguments:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nevergrad</span> <span class="k">as</span> <span class="nn">ng</span>
<span class="c1"># argument transformation</span>
<span class="c1"># Optimization of mixed (continuous and discrete) hyperparameters.</span>
<span class="n">arg1</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">TransitionChoice</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">])</span>  <span class="c1"># 1st arg. = positional discrete argument</span>
<span class="c1"># We apply a softmax for converting real numbers to discrete values.</span>
<span class="n">arg2</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">Choice</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">])</span>  <span class="c1"># 2nd arg. = positional discrete argument</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">Scalar</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">set_mutation</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># the 4th arg. is a keyword argument with Gaussian prior</span>

<span class="c1"># create the parametrization</span>
<span class="c1"># the 3rd arg. is a positional arg. which will be kept constant to &quot;blublu&quot;</span>
<span class="n">instru</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">Instrumentation</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="s2">&quot;blublu&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">instru</span><span class="o">.</span><span class="n">dimension</span><span class="p">)</span>  <span class="c1"># 5 dimensional space</span>
</pre></div>
</div>
<p>The dimension is 5 because:</p>
<ul class="simple">
<li><p>the 1st discrete var. has 1 possible values, represented by a hard thresholding in a 1-dimensional space, i.e. we add 1 coordinate to the continuous problem</p></li>
<li><p>the 2nd discrete var. has 3 possible values, represented by softmax,   i.e. we add 3 coordinates to the continuous problem</p></li>
<li><p>the 3rd var. has no uncertainty, so it does not introduce any coordinate in the continuous problem</p></li>
<li><p>the 4th var. is a real number, represented by single coordinate.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">instru</span><span class="o">.</span><span class="n">set_standardized_data</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">80</span><span class="p">,</span> <span class="o">-</span><span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">instru</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">instru</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">((</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;blublu&#39;</span><span class="p">),</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="mf">7.0</span><span class="p">})</span>
<span class="n">myfunction</span><span class="p">(</span><span class="o">*</span><span class="n">instru</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">instru</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">8.0</span>
</pre></div>
</div>
<p>In this case:
- <code class="code docutils literal notranslate"><span class="pre">args[0]</span> <span class="pre">==</span> <span class="pre">&quot;b&quot;</span></code> because 1 &gt; 0 (the threshold is 0 here since there are 2 values.
- <code class="code docutils literal notranslate"><span class="pre">args[1]</span> <span class="pre">==</span> <span class="pre">&quot;e&quot;</span></code> is selected because proba(e) = exp(80) / (exp(80) + exp(-80) + exp(-80)) = 1
- <code class="code docutils literal notranslate"><span class="pre">args[2]</span> <span class="pre">==</span> <span class="pre">&quot;blublu&quot;</span></code> because it is kept constant
- <code class="code docutils literal notranslate"><span class="pre">value</span> <span class="pre">==</span> <span class="pre">7</span></code> because std * 3 + current_value = 2 * 3 + 1 = 7
The function therefore returns 7 + 1 = 8.</p>
<p>Then you can run the optimization as usual. <code class="code docutils literal notranslate"><span class="pre">PortfolioDiscreteOnePlusOne</span></code> is quite a natural choice when you have a good initial guess and a mix of discrete and continuous variables; in this case, it might be better to use <code class="code docutils literal notranslate"><span class="pre">TransitionChoice</span></code> rather than <code class="code docutils literal notranslate"><span class="pre">Choice</span></code>.
<cite>TwoPointsDE</cite> is often excellent in the large scale case (budget in the hundreds).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nevergrad</span> <span class="k">as</span> <span class="nn">ng</span>
<span class="n">budget</span> <span class="o">=</span> <span class="mi">1200</span>  <span class="c1"># How many episode we will do before concluding.</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;RandomSearch&quot;</span><span class="p">,</span> <span class="s2">&quot;ScrHammersleySearch&quot;</span><span class="p">,</span> <span class="s2">&quot;TwoPointsDE&quot;</span><span class="p">,</span> <span class="s2">&quot;PortfolioDiscreteOnePlusOne&quot;</span><span class="p">,</span> <span class="s2">&quot;CMA&quot;</span><span class="p">,</span> <span class="s2">&quot;PSO&quot;</span><span class="p">]:</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">registry</span><span class="p">[</span><span class="n">name</span><span class="p">](</span><span class="n">parametrization</span><span class="o">=</span><span class="n">instru</span><span class="p">,</span> <span class="n">budget</span><span class="o">=</span><span class="n">budget</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">budget</span> <span class="o">//</span> <span class="mi">3</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="c1"># Ask and tell can be asynchronous.</span>
        <span class="c1"># Just be careful that you &quot;tell&quot; something that was asked.</span>
        <span class="c1"># Here we ask 3 times and tell 3 times in order to fake asynchronicity</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="c1"># The three folowing lines could be parallelized.</span>
        <span class="c1"># We could also do things asynchronously, i.e. do one more ask</span>
        <span class="c1"># as soon as a training is over.</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">myfunction</span><span class="p">(</span><span class="o">*</span><span class="n">x1</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">x1</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># here we only defined an arg, so we could omit kwargs</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">myfunction</span><span class="p">(</span><span class="o">*</span><span class="n">x2</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">x2</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># (keeping it here for the sake of consistency)</span>
        <span class="n">y3</span> <span class="o">=</span> <span class="n">myfunction</span><span class="p">(</span><span class="o">*</span><span class="n">x3</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">x3</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">y3</span><span class="p">)</span>
    <span class="n">recommendation</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">recommend</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;* &quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">&quot; provides a vector of parameters with test error &quot;</span><span class="p">,</span>
          <span class="n">myfunction</span><span class="p">(</span><span class="o">*</span><span class="n">recommendation</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">recommendation</span><span class="o">.</span><span class="n">kwargs</span><span class="p">))</span>
</pre></div>
</div>
<section id="manual-parametrization">
<h3>Manual parametrization<a class="headerlink" href="#manual-parametrization" title="Permalink to this heading"></a></h3>
<p>You always have the possibility to define your own parametrization inside your function (not recommended):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">possible_values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">expx</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="n">probas</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">expx</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">expx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">possible_values</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">possible_values</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probas</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_and_return_test_error_mixed</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">cx</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]]</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="k">if</span> <span class="n">activation</span> <span class="o">!=</span> <span class="s2">&quot;tanh&quot;</span> <span class="k">else</span> <span class="mf">0.</span><span class="p">)</span>

<span class="n">parametrization</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># you can just provide the size of your input in this case</span>

<span class="c1">#This version is bigger.</span>
<span class="k">def</span> <span class="nf">train_and_return_test_error_mixed</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">cx</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)]</span>  <span class="c1"># continuous part.</span>
    <span class="n">presoftmax_values</span> <span class="o">=</span> <span class="n">x</span><span class="p">[(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):]</span>  <span class="c1"># discrete part.</span>
    <span class="n">values_for_this_softmax</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">presoftmax</span><span class="p">:</span>
        <span class="n">values_for_this_softmax</span> <span class="o">+=</span> <span class="p">[</span><span class="n">g</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">values_for_this_softmax</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">dx</span> <span class="o">+=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">values_for_this_softmax</span><span class="p">)</span>
            <span class="n">values_for_this_softmax</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="mf">50.</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">))</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">cx</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span>
            <span class="mi">1</span> <span class="k">if</span> <span class="n">d</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dx</span><span class="p">]</span>

<span class="n">parametrization</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>
</div>
</section>
</section>
<section id="optimization-of-parameters-for-reinforcement-learning">
<h2>Optimization of parameters for reinforcement learning<a class="headerlink" href="#optimization-of-parameters-for-reinforcement-learning" title="Permalink to this heading"></a></h2>
<p>We do not average evaluations over multiple episodes - the algorithm is in charge of averaging, if need be.
<code class="code docutils literal notranslate"><span class="pre">TBPSA</span></code>, based on population-control mechanisms, performs quite well in this case.</p>
<p>If you want to run Open AI Gym, see <a class="reference external" href="https://docs.google.com/document/d/1noubQ_ZTZ4PZeQ1St7Asi1Af02q7k0nRoX_Pipu9ZKs/edit?usp=sharing/">One-line for learning state-of-the-art OpenAI Gym controllers with Nevergrad</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nevergrad</span> <span class="k">as</span> <span class="nn">ng</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Similar, but with a noisy case: typically a case in which we train in reinforcement learning.</span>
<span class="c1"># This is about parameters rather than hyperparameters. TBPSA is a strong candidate in this case.</span>
<span class="c1"># We do *not* manually average over multiple evaluations; the algorithm will take care</span>
<span class="c1"># of averaging or reevaluate whatever it wants to reevaluate.</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimization of parameters in reinforcement learning ===============&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">simulate_and_return_test_error_with_rl</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noisy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="mf">50.</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">))</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span> <span class="o">+</span> <span class="n">noisy</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">()</span>


<span class="n">budget</span> <span class="o">=</span> <span class="mi">1200</span>  <span class="c1"># How many trainings we will do before concluding.</span>


<span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;TwoPointsDE&quot;</span><span class="p">,</span> <span class="s2">&quot;RandomSearch&quot;</span><span class="p">,</span> <span class="s2">&quot;TBPSA&quot;</span><span class="p">,</span> <span class="s2">&quot;CMA&quot;</span><span class="p">,</span> <span class="s2">&quot;NaiveTBPSA&quot;</span><span class="p">,</span>
        <span class="s2">&quot;PortfolioNoisyDiscreteOnePlusOne&quot;</span><span class="p">]:</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">registry</span><span class="p">[</span><span class="n">tool</span><span class="p">](</span><span class="n">parametrization</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">budget</span><span class="o">=</span><span class="n">budget</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">budget</span> <span class="o">//</span> <span class="mi">3</span><span class="p">):</span>
        <span class="c1"># Ask and tell can be asynchronous.</span>
        <span class="c1"># Just be careful that you &quot;tell&quot; something that was asked.</span>
        <span class="c1"># Here we ask 3 times and tell 3 times in order to fake asynchronicity</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">ask</span><span class="p">()</span>
        <span class="c1"># The three folowing lines could be parallelized.</span>
        <span class="c1"># We could also do things asynchronously, i.e. do one more ask</span>
        <span class="c1"># as soon as a training is over.</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">simulate_and_return_test_error_with_rl</span><span class="p">(</span><span class="o">*</span><span class="n">x1</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">simulate_and_return_test_error_with_rl</span><span class="p">(</span><span class="o">*</span><span class="n">x2</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
        <span class="n">y3</span> <span class="o">=</span> <span class="n">simulate_and_return_test_error_with_rl</span><span class="p">(</span><span class="o">*</span><span class="n">x3</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">tell</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">y3</span><span class="p">)</span>

    <span class="n">recommendation</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">recommend</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;* &quot;</span><span class="p">,</span> <span class="n">tool</span><span class="p">,</span> <span class="s2">&quot; provides a vector of parameters with test error &quot;</span><span class="p">,</span>
          <span class="n">simulate_and_return_test_error_with_rl</span><span class="p">(</span><span class="o">*</span><span class="n">recommendation</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">noisy</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="examples-from-our-external-users">
<h2>Examples from our external users<a class="headerlink" href="#examples-from-our-external-users" title="Permalink to this heading"></a></h2>
<p>Nevergrad is a plugin in <a class="reference external" href="https://hydra.cc/docs/plugins/nevergrad_sweeper/">Hydra</a> Facebook’s parameter sweeping library.</p>
<p>Nevergrad is interfaced in <a class="reference external" href="https://iohprofiler.liacs.nl/">IOH Profiler</a>, a tool from Univ. Leiden, CNRS, Sorbonne univ and Tel Hai college for profiling optimization algorithms.</p>
<p>Nevergrad is interfaced in <a class="reference external" href="https://github.com/Foloso/MixSimulator/">MixSimulator</a>, a useful tool to get the optimal parameters for an electrical mix.</p>
<p>Nevergrad is also used for <a class="reference external" href="https://github.com/facebookresearch/nevergrad/blob/main/docs/examples/guiding/Guiding%20image%20generation%20with%20Nevergrad.md/">Guiding latent image generation</a>.</p>
<p>And for <a class="reference external" href="https://github.com/facebookresearch/nevergrad/blob/main/docs/examples/diversity/Diversity%20in%20image%20generation%20with%20Nevergrad.md/">Increasing diversity in image generation</a>.</p>
<p>And for the <a class="reference external" href="https://github.com/facebookresearch/nevergrad/blob/main/docs/examples/lognormal/Lognormal%20mutations%20in%20Nevergrad.md/">Detection of fake images</a>.</p>
<p>And for <a class="reference external" href="https://github.com/facebookresearch/nevergrad/blob/main/docs/examples/retrofitting/Retrofitting%20with%20Nevergrad.md">Retrofitting</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="parametrization.html" class="btn btn-neutral float-left" title="Parametrizing your optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="optimizers_ref.html" class="btn btn-neutral float-right" title="Optimizers API Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Facebook AI Research.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>